\begin{abstract}
We will offer a new approach for solving the problem of two stage constrained submodular maximization. Our solution offers better results as well faster runtime compared to previous algorithms devised for this task. Historically, submodular maximization has represented a topic of interest in Computer Science and Machine Learning, especially in data summarization. The single stage submodular problem is solvable by a fast greedy algorithm with proven theoretical guarantees. The two stage problem adds a layer of complexity to this task, and was previously tackled in \cite{balkanski16learning}. We improve on their approach, offering a $(1 - 1/e) / 2$ approximation ration for general monotone submodular functions. 

\end{abstract}