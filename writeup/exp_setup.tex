\section{Experiment Setup}
\label{sec::exp_setup}

In the next section, we present empirical evidence for the performance of our algorithm. Here we describe the four other routines we test against alongside the datasets we considered. 

\subsection{Routines}

\subsubsection{Our algorithm}

TODO: should we mention this?

\subsubsection{Local Search}

This is the algorithm used in \cite{balkanski16learning}. \\
\\
TODO: sketch it here?? How detailed? \\
\\

\subsubsection{Greedy Sum}

We will greedily pick the element that maximizes our objective function as though the cardinality constraint in the second stage were $l$. In other words, at each one of the $l$ steps we'll maximize

$$
\sum_{i = 1}^{m} \max_{\substack{x \in \Omega \\ x \notin S }} f_i(S \cup \{ x \}) -
\sum_{i = 1}^{m} f_i(S)
$$

At the end of this procedure we have $T_1 = \dots = T_m = S$. To reinstate the $k$ cardinality constraint in the second stage, we will perform submodular maximization on the elements in $S$ for each of the $m$ categories. 

\subsubsection{Greedy Merge}

We will ingore the first stage constraint on $l$, and perform constrained submodular maximization for each function subject to $T_i \leq k$. Our solution set $S$ will be the union of all the $T_i$. The point of this approach is that even if we fail to obtain a set of cardinality at most $l$, we do however get a good estimate for an upper bound. This is not a tight upper bound however, as the greedy solution only guarantees a $1-1/e$ approximation of the optimal (TODO citation). Thus, for large values of $l$ this approach can be outperformed by the other routines. 

\subsubsection{Clustering}

TODO: we might want to omit due to poor perf? \\
\\
We perform \textit{k-means} clustering. We pick $l$ cluster centers, in the idea that these would give a good summary of our data. We then perform single stage constrained submodular maximization and pick elements for each set $T_i$ from these $l$ cluster centers.

\subsection{Datasets and choices for \textit{f}}

TODO: should probably make these more detailed. On the right track? need links to datasets and citations in bib. 

\subsubsection{Wikipedia}

We run our code alongside the code from \cite{balkanski16learning} on a Wikipedia ML dataset. For this experiment we choose $f$ to be coverage functions.

\subsubsection{Movielens}

We analyze part of the Movielens 100k (TODO check) dataset. In our experiments we set $n,m,l,k$ to ????. We set our functions $f$ to the \textit{Facility Location} function from (TODO cite paper in foler). We compute the similarity between two movies based on a precomputed similarity matrix that we input to our algorithm. TODO: more detalis about this?

\subsubsection{Image Processing}

We consider part of the VOC2012 dataset. The dataset contains images associated with various tags. For each image, we create a feature vector out of the tags associated with said image, and use Facility Location as our function of choice. Similarity is defined as the $2-$norm between the feature vectors. We ran experiments for $n = 100, m = 20, l = 8, k = 5$. TODO: can rerun. right ballpark?

\subsubsection{Article Summarization}

We consider the reuters21578 dataset. This is an article database, each article having some keywords associated with it. We again use Facility Location, and compute similarity through cosine similarity. Our experiments have $n = 126, m = 35, l = 10, k = 5$. \\
\\
TODO: Should probably standardize experiments somewhat?? At least $l$ and $k$, as $m$ is forced for image proc and art summarization (we could change it but is tedious and might not be worth it)

