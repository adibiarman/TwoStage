\section{Introduction}
\label{sec::intro}

Our paper is focused on solving data summarization problems - given a set of elements (images, news articles, movies and their ratings) and several categories these elements are part of, we are interested in a selecting a subset of the original set such that each category is well represented in said subset. For our problem, both the subset we are selecting and the number of elements in the subset that we can use for each category will come under cardinality constraints. \\
\\
TODO: Paragraph on history of submodularity research.. \\
\\
We consider the setup in \cite{balkanski16learning}. Let us consider we have a set of $n$ elements, each element being assigned to certaing categories. There are $m$ such categories, and we can measure the relelvance of a set with respect to a category through $m$ submodular functions, each category having a corresponding function. We first want to pick a ground set $S$ of size at most $l$ from the $n$ elements at our disposal. Then, for each category, we will be able to choose a set $S_i \subset S$ such that $S_i$ has size at most $k$. Hence the two stage problem. \\
\\
The setup for this problem allows us to tackle a much wider gamut of problems compared to the single stage constrained submodular maximization. Some examples, which we will later describe in this paper, include image processing (finding a subset of images most relevant to certaing features), movie recommendation (based on user preference, what movies can we select such that we make a good selection of family, adventure, horror etc. films) or article summarization (given a corpus of articles, find a subset that are relevant to several news categories). When running these experiemnts, we will compare performance to the algorithm described in \cite{balkanski16learning}, and show that our approach is superior in runtime and similar, if not better in objective value. \\
\\
TODO: Paragraph about structure of paper 

