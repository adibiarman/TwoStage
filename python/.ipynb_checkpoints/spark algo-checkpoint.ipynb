{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingesting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is responsible for ingesting the data from\n",
    "\n",
    "https://snap.stanford.edu/data/egonets-Facebook.html\n",
    "\n",
    "into spark. This is an ego-network. Such a network is defined here http://www.analytictech.com/networks/egonet.htm:\n",
    "\n",
    "Ego networks consist of a focal node (\"ego\") and the nodes to whom ego is directly connected to (these are called \"alters\") plus the ties, if any, among the alters. Of course, each alter in an ego network has his/her own ego network, and all ego networks interlock to form The human social network.\n",
    "\n",
    "A description of the dataset can be found here https://snap.stanford.edu/data/readme-Ego.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the names corresponding to the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "features = dict()\n",
    "\n",
    "featNames = open(\"./facebook/0.featnames\", \"r\")\n",
    "\n",
    "while True:\n",
    "    line = featNames.readline()\n",
    "    \n",
    "    if line == '':\n",
    "        break\n",
    "    \n",
    "    featCat = line.split(\";\")[0]\n",
    "\n",
    "    featInd = int(featCat.split(\" \")[0])\n",
    "    featName = featCat.split(\" \")[1]\n",
    "    \n",
    "    if featName in features:\n",
    "        features[featName].append(featInd)\n",
    "    else:\n",
    "        features[featName] = [featInd]\n",
    "    \n",
    "print features[\"birthday\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, store a N x [# features] matrix decribing the vertices and their corresponding features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featMat = []\n",
    "\n",
    "nodeDetails = open(\"./facebook/0.feat\", \"r\")\n",
    "\n",
    "while True:\n",
    "    line = nodeDetails.readline()\n",
    "    \n",
    "    if line == '':\n",
    "        break\n",
    "        \n",
    "    data = line.split()\n",
    "    \n",
    "    # remove the vertex index\n",
    "    data.pop(0)\n",
    "    \n",
    "    intData = [int(d) for d in data]\n",
    "    \n",
    "    featMat.append(intData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347   224\n"
     ]
    }
   ],
   "source": [
    "numNeighbors = len(featMat)\n",
    "numFeatures = len(featMat[0])\n",
    "print numNeighbors, \" \", numFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, our starting node has 347 neighbors, each neighbor having 0,1,2 or more of the 224 available features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to process the circle data. A circle contains the ego node and all the other nodes appearing on a specific line. We'll store this data as a dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[71, 215, 54, 61, 298, 229, 81, 253, 193, 97, 264, 29, 132, 110, 163, 259, 183, 334, 245, 222]\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "circles = dict()\n",
    "\n",
    "circleDetails = open(\"./facebook/0.circles\", \"r\")\n",
    "\n",
    "ind = 0\n",
    "\n",
    "while True:\n",
    "    line = circleDetails.readline()\n",
    "    \n",
    "    if line == '':\n",
    "        break\n",
    "        \n",
    "    data = line.split()\n",
    "    data.pop(0)\n",
    "    \n",
    "    data = [int(d) for d in data]\n",
    "    \n",
    "    circles[ind] = data\n",
    "    \n",
    "    ind = ind + 1\n",
    "\n",
    "print circles[0]\n",
    "print len(circles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found 24 circles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've parsed all the data - feature & circle information. \n",
    "\n",
    "To recap\n",
    "\n",
    "- feature names are stored in 'features'. features[\"feature_name\"] will return a list of vertices having that feature\n",
    "- feature information is stored in the binary matrix 'featMat'. featMat[i,j] = 0 if node i doesn't have feature j and 1 otherwise\n",
    "- circle information is stored in 'circles'. circle[i] returns a list of vertices being part of a specific circle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Setting up the two stage submodular problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this graph, we'll set up $F$ as being a sum of coverage functions.\n",
    "\n",
    "Each $f_i$ will be a coverage function corresponding to feature name $i$. So, for example $f_{birthday}$. In this example, there are $8$ features contained in the $birthday$ category. So, for a given set $S$, the value of $f_{birthday}$ will be the number of features from $0$ to $7$ that are covered by at least one vertex in $S$.\n",
    "\n",
    "So, we are ignoring circles for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation will be done in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = sc.parallelize(range(10), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how to build a function that takes in arguments and passes them further down to MapPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def g(const):\n",
    "    def newf(partition):\n",
    "        fres = []\n",
    "        \n",
    "        for num in partition:\n",
    "            if num % 2 == 0:\n",
    "                fres.append(num * const)\n",
    "\n",
    "        return fres\n",
    "\n",
    "    return newf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 4, 6, 8]\n"
     ]
    }
   ],
   "source": [
    "print test.mapPartitions(g(1)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 12, 24, 36, 48]\n"
     ]
    }
   ],
   "source": [
    "print test.mapPartitions(g(6)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll write our algorithm inside a curried function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doComputations(features, featMat, k, l):\n",
    "    n = len(featMat)\n",
    "    m = len(features)\n",
    "    \n",
    "    # a callable f for each feature name\n",
    "    def f()\n",
    "    \n",
    "    # a \n",
    "    def novel(partition):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
